{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear Hiring Manager Ecowiser,\n",
    "\n",
    "I am writing to express my sincere gratitude for the opportunity you provided me to scrape LinkedIn data as an assignment. It was a valuable learning experience and I am grateful for the chance to work on such an interesting assignment.\n",
    "\n",
    "However, I regret to inform you that I was unable to complete the assignment as requested. After researching LinkedIn's terms and conditions and their policies, I discovered that their rules forbid using any automated methods to gather data from its platform, including scraping, crawling, data mining, and so on. LinkedIn employs a number of technical safeguards, including rate restrictions, CAPTCHAs, and IP blocking to limit illegal access to its platform and automated data scraping. These measures are meant to guarantee that only authorized users have access to the platform and its data.\n",
    "\n",
    "Scraping data from LinkedIn without permission can result in consequences such as having my account restricted or banned. Additionally, scraped data can be gathered from multiple sites, combed, and sold in large batches, to be used for phishing and other campaigns designed to trick you into sharing private information. As a result, I decided not to proceed with the assignment in order to avoid any potential consequences.\n",
    "\n",
    "However, I am still eager to prove my potential and would appreciate the opportunity to work on another assignment. If possible, could you please provide me with another project that would allow me to demonstrate my skills and abilities? Here is a sample pseudo code that I have tried to showcase my skills\n",
    "\n",
    "Once again, thank you for this wonderful opportunity. I appreciate your understanding and look forward to the possibility of working with you again in the future.\n",
    "\n",
    "Sincerely,<br>\n",
    "Abdulkadar Kapasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Web Scraping Script 1: LinkedIn User Profiles**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from parsel import selector\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.edge.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = Options()\n",
    "service = Service(executable_path = \"C:/Users/Abdulkadar/OneDrive/Downloads/edgedriver_win64/msedgedriver.exe\")\n",
    "\n",
    "# Setting up webdriver\n",
    "driver = webdriver.Edge(option = option, service = service)\n",
    "\n",
    "# Function to ensure all key data fields have a value\n",
    "def validate_field(field):\n",
    "    # if field i present pass if field\n",
    "    if field:\n",
    "        pass\n",
    "    # if field is not present print text else\n",
    "    else:\n",
    "        field = \"No Result\"\n",
    "    return field\n",
    "\n",
    "# driver.get() method will navigate to a page given by the url address\n",
    "driver.get(\"https://www.linkedin.com/\")\n",
    "\n",
    "# locate email form by_class_name\n",
    "username = driver.find_element(By.ID, \"session_key\")\n",
    "# user email\n",
    "username.send_keys(\"###########\") # send_keys() to simulate key strokes\n",
    "\n",
    "sleep(0.5) # sleep for 5 seconds\n",
    "\n",
    "# locate password form by_class_name\n",
    "password = driver.find_element(By.ID, \"session_password\")\n",
    "# user password\n",
    "password.send_keys(\"########\") # send_keys() to simulate key strokes\n",
    "\n",
    "sleep(0.5)\n",
    "\n",
    "# locate submit button by_xpath\n",
    "sign_in_button = driver.find_element(By.XPATH, '//*[@type=\"submit\"]')\n",
    "\n",
    "# .click() to mimic button click\n",
    "sign_in_button.click()\n",
    "sleep(15)\n",
    "\n",
    "\n",
    "# Taking input from the user\n",
    "firstname = input(\"Enter first name: \")\n",
    "lastname = input(\"Enter last name: \")\n",
    "\n",
    "# To change search pages from 1 - 10\n",
    "for i in range(11):\n",
    "\n",
    "    url = f\"https://www.linkedin.com/search/results/people/?keywords={firstname}%20{lastname}&origin=CLUSTER_EXPANSION&page={i}&sid=ho3\"\n",
    "\n",
    "    # Search for the user after logging in\n",
    "    search_bar = driver.find_element_by_name('search')\n",
    "    search_bar.send_keys(f\"{firstname} {lastname}\")\n",
    "    search_bar.submit()\n",
    "\n",
    "    # Wait for the search results page to load\n",
    "    driver.implicitly_wait(25) # Here wait time is in seconds can be adjusted\n",
    "\n",
    "    # Getting the page source code (HTML) and parse it in beautifulsoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Extracting data for the first 5 results\n",
    "    data = []\n",
    "    profiles = soup.find_all('div', class_ = \"search-result__info\")\n",
    "    for profile in profiles[:5]:\n",
    "        # I have kept the url string blank \n",
    "        name = profile.find_elements(By.XPATH, '').text.lower().strip()\n",
    "        title = profile.find_elements(By.XPATH, '').text.lower().strip()\n",
    "        location = profile.find_elements(By.XPATH, '').text.lower().strip()\n",
    "\n",
    "        # For Every profile the Profile Id gets changed so that's why this method won't work [@id=\"Kl4wwIsnSpeTB26y8sZZzA==]\n",
    "        \n",
    "        # name = profile.find_elements(By.XPATH, '//*[@id=\"Kl4wwIsnSpeTB26y8sZZzA==\"]/div/ul/li[1]/div/div/div[2]/div/div[1]/div/span[1]/span/a/span/span[1]/text()').text.lower().strip()\n",
    "        # title = profile.find_elements(By.XPATH, '//*[@id=\"Kl4wwIsnSpeTB26y8sZZzA==\"]/div/ul/li[1]/div/div/div[2]/div/div[2]/div[1]/text()').text.lower().strip()\n",
    "        # location = profile.find_elements(By.XPATH, '//*[@id=\"Kl4wwIsnSpeTB26y8sZZzA==\"]/div/ul/li[1]/div/div/div[2]/div/div[2]/div[2]/text()').text.lower().strip()\n",
    "        data.append([name, title, location])\n",
    "\n",
    "    # Creating a dataframe\n",
    "    df = pd.DataFrame(data, columns = [\"Name\", \"Title\", \"Location\"])\n",
    "\n",
    "    # Save to csv\n",
    "    df.to_csv(\"LinkedIn_data.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Web Scraping Script 2: LinkedIn User Profiles**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Edge()\n",
    "\n",
    "# Sign in to LinkedIn\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "username = \"############\"\n",
    "password = \"######\"\n",
    "driver.find_element(By.ID, \"username\").send_keys(username)\n",
    "driver.find_element(By.ID, \"password\").send_keys(password)\n",
    "driver.find_element(By.XPATH, \"//button[text()='Sign in']\").click()\n",
    "\n",
    "input(\"Press Enter after completing the CAPTCHA...\")\n",
    "\n",
    "# Scrape LinkedIn search results\n",
    "search_query = \"firstname lastname\"\n",
    "search_results = []\n",
    "\n",
    "for page in range(1, 11):  # Scraping 10 pages of search results\n",
    "    search_url = f\"https://www.linkedin.com/search/results/people/?keywords={search_query}&page={page}\"\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)  # Give the page some time to load (you can adjust this)\n",
    "\n",
    "    profiles = driver.find_elements(By.CLASS_NAME, \"search-result__info\")\n",
    "    for profile in profiles[:5]:  # Scraping the first 5 profiles on each page\n",
    "        name = profile.find_element(By.CLASS_NAME, \"display-flex\").text\n",
    "        headline = profile.find_element(By.CLASS_NAME, \"entity-result__primary-subtitle t-14 t-black t-normal\").text\n",
    "        location = profile.find_element(By.CLASS_NAME, \"entity-result__secondary-subtitle t-14 t-normal\").text\n",
    "        search_results.append({\"Name\": name, \"Headline\": headline, \"Location\": location})\n",
    "\n",
    "# Convert the scraped data into a DataFrame\n",
    "df = pd.DataFrame(search_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"linkedin_search_results.csv\", index=False)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear Hiring Manager Ecowiser,\n",
    "\n",
    "I am writing to inform you that I am unable to use the LinkedIn API for the assignment which was assigned to me. After researching LinkedIn's API terms and conditions and their policies, I discovered that their rules require explicit approval from LinkedIn for most permissions and partner programs. Applications must be authorized and authenticated before they can fetch data from LinkedIn or get access to member data.\n",
    "\n",
    "Using the LinkedIn API without permission can result in consequences such as having your application restricted or banned. As a result, I have decided not to proceed with using the LinkedIn API in order to avoid any potential consequences.\n",
    "\n",
    "I want you to know that I have tried many different ways to complete the assignment. I have taken help from various sources such as Google, YouTube, and Stack Overflow, but unfortunately, I haven't found any relevant solution for this assignment.\n",
    "\n",
    "However, I am still eager to prove my potential and would appreciate the opportunity to work on another assignment. If possible, could you please provide me with another project that would allow me to demonstrate my skills and abilities? Here is a sample pseudo code that I have tried to showcase my skills\n",
    "\n",
    "I apologize for any inconvenience this may cause and I am willing to discuss alternative solutions for the same.\n",
    "Once again, thank you for this wonderful opportunity. I appreciate your understanding and look forward to the possibility of working with you again in the future.\n",
    "\n",
    "Sincerely,<br>\n",
    "Abdulkadar Kapasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Web Scraping Script 3: LinkedIn API**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have tried with my original details but it couldn't work\n",
    "api_key = '#########'\n",
    "api_secret_key = '###########'\n",
    "api_access_token = '####################'\n",
    "\n",
    "first_name = input('Enter the first name: ')\n",
    "last_name = input('Enter the last name: ')\n",
    "\n",
    "# Encode the first name and last name for the LinkedIn API request\n",
    "encoded_firstname = first_name.replace(' ', '%20')\n",
    "encoded_lastname = last_name.replace(' ', '%20')\n",
    "\n",
    "# Define the LinkedIn API URL for searching by name with a maximum of 10 results\n",
    "url = f\"https://api.linkedin.com/v2/people?q=firstName:{encoded_firstname},lastName:{encoded_lastname}&start=0&count=10\"\n",
    "\n",
    "# Set the headers with the access token\n",
    "headers = {'Authorization': f'Bearer {api_access_token}'}\n",
    "\n",
    "# Send a GET request to the LinkedIn API\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract and create a list of dictionaries for the first 5 profiles\n",
    "    profiles_data = []\n",
    "    for person in data.get('elements', [])[:5]:\n",
    "        name = f\"{person.get('firstName', 'N/A')} {person.get('lastName', 'N/A')}\"\n",
    "        title = person.get('headline', 'N/A')\n",
    "        location = person.get('location', {}).get('preferredGeoPlaceName', 'N/A')\n",
    "        profiles_data.append({'Name': name, 'Title': title, 'Location': location})\n",
    "    \n",
    "    df = pd.DataFrame(profiles_data)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "else:\n",
    "    print(\"Error: {} - Unable to fetch data via LinkedIn API\".format(response.status_code))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
